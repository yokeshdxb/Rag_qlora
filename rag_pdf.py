# -*- coding: utf-8 -*-
"""RAG_PDF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KrZp-Ze19keGzXzA9Eqc3oWAIDdqHWlh

# **Basic RAG using LangChain**
"""

! pip install sentence_transformers
! pip install pypdf
! pip install faiss-cpu
! pip install langchain
! pip install langchain-openai
! pip install langchain-community

# load pdf
from langchain_community.document_loaders import PyPDFLoader
loader = PyPDFLoader("/content/qlora_paper.pdf")
documents = loader.load()

# split document content
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter()
text = text_splitter.split_documents(documents)

# load embedding model
from langchain.embeddings import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5", encode_kwargs = {"normalize_embeddings": True})

# create vectorstore using FAISS
from langchain.vectorstores import FAISS
vectorstore = FAISS.from_documents(text, embeddings)

# saving the vectorstore
vectorstore.save_local("vectorstore.db")

# create retriever
retriever = vectorstore.as_retriever()

import os
os.environ["OPENAI_API_KEY"] = "your Open API"
print(os.getenv("OPENAI_API_KEY"))

# load llm
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name="gpt-3.5-turbo")

# create document chain

from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

template = """"
You are an assistant for question-answering tasks.
Use the provided context only to answer the following question:

<context>
{context}
</context>

Question: {input}
"""
prompt = ChatPromptTemplate.from_template(template)
doc_chain = create_stuff_documents_chain(llm, prompt)

# create retrieval chain
from langchain.chains import create_retrieval_chain
chain = create_retrieval_chain(retriever, doc_chain)

response = chain.invoke({"input": "what is Qlora?"})

response['answer']

def get_qlora_response(prompt: str) -> None:
    """
    Generate a response using QLoRA-based model.

    Parameters:
    prompt (str): The user's input/question for the model.

    Returns:
    None: Prints the model's response.
    """
    # Assuming the vector store is already built and saved, load it
    from langchain.vectorstores import FAISS
    vectorstore = FAISS.from_documents(text, embeddings)

    vectorstore.save_local("vectorstore.db")
    retriever = vectorstore.as_retriever()

    # Retrieve the relevant documents based on the input prompt
    retrieved_docs = retriever.get_relevant_documents(prompt)

    # Combine the retrieved documents with the prompt (context)
    context = "\n".join([doc.page_content for doc in retrieved_docs])  # Format the retrieved docs as text
    full_prompt = f"{context}\n\n{prompt}"

    # Generate the response using QLoRA and retrieved docs
    response = llm.predict(text=full_prompt)

    #check if the response length exceeds
    max_length =500
    if len(response) > max_length:
      mid_point = len(response) // 2
      response_part1 = response[:mid_point]
      response_part2 = response[mid_point:]
      print("Chatbot (part1):", response_part1)
      print("Chatbot (part2):", response_part2)
    else:
      print("Chatbot:", response)

# Start interactive loop
while True:
    user_input = input("You: ")
    if user_input.lower() in ['bye', 'quit', 'exit']:
        print("Chatbot: Goodbye!")
        break
    get_qlora_response(user_input)

!pip install streamlit

import streamlit as st
openai_api_key = st.secrets["OPENAI_API_KEY"]
print(openai_api_key)

import streamlit as st
openai_api_key = st.secrets["OPENAI_API_KEY"]
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS

# Load the QLoRA model (or any fine-tuned language model)
llm = ChatOpenAI(model_name="gpt-3.5-turbo")

# Load FAISS vector store
vectorstore = FAISS.load_local("vectorstore.db")
retriever = vectorstore.as_retriever()

def get_qlora_response(prompt: str) -> str:
    """
    Generate a response using QLoRA-based model and return the response.

    Parameters:
    prompt (str): The user's input/question for the model.

    Returns:
    str: The model's response, possibly split if too long.
    """
    # Retrieve the relevant documents based on the input prompt
    retrieved_docs = retriever.get_relevant_documents(prompt)

    # Combine the retrieved documents with the prompt (context)
    context = "\n".join([doc.page_content for doc in retrieved_docs])  # Access the text content
    full_prompt = f"{context}\n\n{prompt}"

    # Generate the response using QLoRA and the combined prompt
    response = llm.predict(text=full_prompt)

    # Check if the response length exceeds a certain limit, and split it
    max_length = 500  # Maximum length before splitting
    if len(response) > max_length:
        mid_point = len(response) // 2
        response_part1 = response[:mid_point]
        response_part2 = response[mid_point:]

        return f"Chatbot (Part 1): {response_part1}\n\nChatbot (Part 2): {response_part2}"
    else:
        return f"Chatbot: {response}"

# Streamlit app UI
st.title("QLoRA Chatbot")
st.write("Ask anything about QLoRA or any other topic you're curious about!")

# Text input from the user
user_input = st.text_input("You:", "")

if user_input:
    # Get response from QLoRA model
    answer = get_qlora_response(user_input)
    st.write(answer)