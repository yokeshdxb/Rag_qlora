PDF RAG App Â· FAISS + LangChain

This app builds a Retrieval-Augmented Generation (RAG) system that ingests PDFs, indexes them with FAISS, and answers questions by retrieving relevant document snippets.

âœ¨ Features
	â€¢	ğŸ“„ Ingest multiple PDFs from Docs/ (recursive)
	â€¢	ğŸ” Chunking + embeddings â†’ FAISS vector index on disk
	â€¢	ğŸ§  Simple question-answering based on indexed content
	â€¢	âš¡ Caching & configurable chunk sizes/overlap
	â€¢	ğŸ§ª Simple evaluation harness (manual Q/A set)

â¸»

ğŸ§± Project Structure

.
â”œâ”€ ingest.py                  # Ingest PDFs â†’ FAISS index
â”œâ”€ rag_chain.py               # Chains, prompts, retrieval logic
â”œâ”€ utils.py                   # Helpers: I/O, text utils, caching
â”œâ”€ requirements.txt
â”œâ”€ Docs/                      # Put your PDFs here
â”œâ”€ index/                     # Persisted FAISS store
â”œâ”€ eval/
â”‚   â”œâ”€ questions.jsonl        # [{"question": "...", "answer": "..."}]
â”‚   â””â”€ run_eval.py
â””â”€ README.md


â¸»

âš™ï¸ Prerequisites
	â€¢	Python 3.10+
	â€¢	Basic build tools (for FAISS CPU)

â¸»

ğŸ”‘ Environment Variables

Use one of the following:

Option A: .env file (local dev)

EMBED_MODEL=text-embedding-3-small
CHUNK_SIZE=1200
CHUNK_OVERLAP=150
TOP_K=4


â¸»

ğŸ“¦ Install

# Create venv
python -m venv .venv
# macOS/Linux
source .venv/bin/activate
# Windows
.\.venv\Scripts\activate

# Install deps
pip install -r requirements.txt

Minimal requirements.txt (edit if you already have one):

langchain
faiss-cpu
pypdf
python-dotenv


â¸»

ğŸ“¥ Ingest PDFs

Put all PDFs in Docs/ then run:

python ingest.py

This will:
	1.	Load PDFs from Docs/ (recursively)
	2.	Chunk & embed
	3.	Save FAISS index under index/

â¸»

ğŸ§© How It Works (RAG Pipeline)
	1.	Load PDFs â†’ PyPDFLoader
	2.	Split documents â†’ RecursiveCharacterTextSplitter
	3.	Embed chunks â†’ custom embedding model (e.g., OpenAI, HuggingFace)
	4.	Index with FAISS (persisted)
	5.	Retrieve top-K chunks per query
	6.	Generate answer based on retrieved context
	7.	Cite sources with page numbers and snippets

Prompt (simplified) in rag_chain.py:
	â€¢	System: â€œAnswer with citations from provided context; say â€˜I donâ€™t knowâ€™ if not found.â€
	â€¢	User: question
	â€¢	Context: top-K chunks (title, page, snippet)

â¸»

ğŸ§ª Quick Eval (Optional)

Add a few Q/A pairs to eval/questions.jsonl:

{"question":"What is the warranty policy?", "answer":"..."}

Run:

python eval/run_eval.py

Outputs naive accuracy & saves model answers for review.

â¸»

ğŸš€ Deployment (Optional)

For server or local deployment, you can create a simple web service or run the app locally using Flask or FastAPI to handle incoming questions.

â¸»

ğŸ› ï¸ Configuration

Tweak via env vars or constants in rag_chain.py:
	â€¢	CHUNK_SIZE, CHUNK_OVERLAP â†’ controls recall/latency
	â€¢	TOP_K â†’ retrieval breadth
	â€¢	EMBED_MODEL â†’ embedding model

â¸»

ğŸ”§ Troubleshooting
	â€¢	FAISS not found â†’ ensure faiss-cpu installed (no GPU needed)
	â€¢	No answers / hallucinations â†’ increase TOP_K, reduce temperature, improve chunk size
	â€¢	Duplicate chunks â†’ clean Docs/, delete index/, re-ingest
	â€¢	Large PDFs slow â†’ raise CHUNK_SIZE moderately and cache embeddings

â¸»

ğŸ—ºï¸ Roadmap
	â€¢	Add reranking (e.g., BGE-Reranker)
	â€¢	Add citations highlighting in UI
	â€¢	Add confidence scoring & feedback buttons
	â€¢	Add basic eval dashboard

â¸»

ğŸ“¸ Screenshots (optional)

/assets/
  home.png
  answer_with_citations.png

Embed in README once you capture them:

![Home](assets/home.png)
![Answer](assets/answer_with_citations.png)


â¸»

ğŸ“ License

MIT â€” feel free to use and modify.

â¸»

ğŸ™ Acknowledgements
	â€¢	LangChain
	â€¢	FAISS
